{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('E:/Data Science/ML/ud120-projects/tools/')\n",
    "sys.path.append('E:/Data Science/ML/ud120-projects/choose_your_own')\n",
    "sys.path.append('E:/Data Science/ML/ud120-projects/datasets_questions')\n",
    "\n",
    "sys.path.append('E:/Data Science/ML/ud120-projects/maildir/maildir')\n",
    "\n",
    "import os\n",
    "os.chdir('E:/Data Science/ML/ud120-projects/text_learning')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "string1 = '''Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec pharetra ornare lacinia. \n",
    "Cras et facilisis sem. Mauris fermentum nec dui at aliquam. Mauris ante justo, aliquam \n",
    "eget facilisis ac, elementum eget dolor. Nulla lacinia ac lorem vel tempus. Vivamus condimentum \n",
    "vulputate semper. Nullam dictum, mauris dignissim auctor suscipit, dui turpis elementum turpis, \n",
    "et tempor urna diam sed nisl. Vestibulum mollis quam at nisl egestas tincidunt. Nullam ut nibh \n",
    "fringilla, ultricies neque vel, tempor erat. Phasellus vel sem vitae ligula viverra luctus hendrerit \n",
    "ac quam. Vivamus dignissim, diam sed porttitor lacinia, elit erat lobortis tellus, nec euismod dolor turpis a leo.'''\n",
    "\n",
    "string2 = '''Mauris felis nibh, tempor ac pulvinar sed, feugiat at neque. In enim elit, venenatis nec magna eu, vestibulum \n",
    "lobortis libero. Nullam scelerisque pulvinar ex consectetur tempor. Morbi congue quis leo auctor facilisis. Mauris \n",
    "et diam ultricies, interdum nisi ac, condimentum turpis. Donec a risus sed dolor blandit ultrices. Nulla ac ultrices \n",
    "elit. Nulla dictum metus tortor, in sollicitudin enim rutrum id. Nulla magna felis, molestie vel odio et, congue ornare \n",
    "nisi. Nunc molestie, mi non blandit sodales, neque diam varius quam, nec tempus metus neque vel nibh. Morbi sit amet \n",
    "sapien nec ipsum feugiat lacinia ut eu orci. Mauris id enim tincidunt, aliquet augue quis, vehicula libero. Donec eu \n",
    "laoreet nibh.'''\n",
    "\n",
    "string3 = '''Donec urna massa, faucibus et interdum id, facilisis interdum augue. Nunc enim nulla, tristique sit amet velit \n",
    "in, lacinia mollis mi. Nullam malesuada felis sed libero porttitor dapibus ut quis dolor. Curabitur vitae neque \n",
    "at arcu condimentum suscipit. Quisque sollicitudin est a elit sollicitudin, ac consectetur nisi blandit. Vestibulum \n",
    "rhoncus viverra orci, et porta purus maximus quis. Pellentesque consectetur velit eget orci rutrum mollis. Nullam \n",
    "condimentum vehicula ante at dignissim. Curabitur gravida, lorem in vehicula gravida, enim arcu semper nulla, \n",
    "id tempor magna eros vel tortor.'''\n",
    "\n",
    "email_list = [string1, string2, string3]\n",
    "\n",
    "bag_of_words = vectorizer.fit_transform(email_list)\n",
    "\n",
    "print vectorizer.vocabulary_.get('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Stopwords from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\song\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "sw = stopwords.words('english')\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming Up with parseOutText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi Everyone  If you can read this message youre properly using parseOutText  Please proceed to the next part of the project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        words = text_string\n",
    "        \n",
    "    return words\n",
    "    \n",
    "\n",
    "ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "text = parseOutText(ff)\n",
    "print text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def parseOutText(f):\n",
    "    '''\n",
    "    Input: a file containing text\n",
    "    \n",
    "    Output: the stemmed words in the input text, all separated by a single space\n",
    "    '''\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    \n",
    "    # the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    # the string of words\n",
    "    words = \"\"\n",
    "    \n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        for word in text_string.split():\n",
    "            # stem the word and add it to words\n",
    "            words += stemmer.stem(word) + ' '       \n",
    "        \n",
    "    return words[:-1]\n",
    "    \n",
    "\n",
    "ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "text = parseOutText(ff)\n",
    "print text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
