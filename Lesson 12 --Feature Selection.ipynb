{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('E:/Data Science/ML/ud120-projects/tools/')\n",
    "sys.path.append('E:/Data Science/ML/ud120-projects/choose_your_own')\n",
    "sys.path.append('E:/Data Science/ML/ud120-projects/datasets_questions')\n",
    "\n",
    "import os\n",
    "os.chdir('E:/Data Science/ML/ud120-projects/feature_selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training points = 150\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "words_file = 'E:/Data Science/ML/ud120-projects/feature_selection/word_data_overfit.pkl' \n",
    "authors_file = 'E:/Data Science/ML/ud120-projects/feature_selection/email_authors_overfit.pkl'\n",
    "    \n",
    "def my_func(words_file, authors_file):\n",
    "    '''\n",
    "    I will use this code later in the lesson so I made it a function\n",
    "    '''\n",
    "    \n",
    "    ### The words (features) and authors (labels), already largely processed.\n",
    "    ### These files should have been created from the previous (Lesson 10)\n",
    "    ### mini-project.\n",
    "    word_data = pickle.load( open(words_file, 'r'))\n",
    "    authors = pickle.load( open(authors_file, 'r') )\n",
    "\n",
    "\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set (the\n",
    "    ### remainder go into training)\n",
    "    ### feature matrices changed to dense representations for compatibility with\n",
    "    ### classifier functions in versions 0.15.2 and earlier\n",
    "    from sklearn import cross_validation\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train = vectorizer.fit_transform(features_train)\n",
    "    features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "    ### a classic way to overfit is to use a small number\n",
    "    ### of data points and a large number of features;\n",
    "    ### train on only 150 events to put ourselves in this regime\n",
    "    features_train = features_train[:150].toarray()\n",
    "    labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(features_train, labels_train)\n",
    "    \n",
    "    return clf, vectorizer, features_train, features_test, labels_train, labels_test\n",
    "\n",
    "\n",
    "(clf, vectorizer, features_train, features_test, labels_train, labels_test) = my_func(words_file, authors_file)\n",
    "\n",
    "print 'Number of training points = {0}'.format(len(features_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of Your Overfit Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95904436860068254"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the Most Powerful Features and Use TfIdf to Get the Most Important Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33604, 0.76470588235294124, u'sshacklensf')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_features = [(number, feature, vectorizer.get_feature_names()[number]) for number, feature in \n",
    "                zip(range(len(clf.feature_importances_)), clf.feature_importances_) if feature > 0.2]\n",
    "top_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
